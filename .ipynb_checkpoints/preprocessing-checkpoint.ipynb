{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908e2c54-e904-45b2-964d-8cc53e44134a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f275845-fda2-4bda-947b-062339d8bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Setup\n",
    "import os\n",
    "\n",
    "# Data Structures \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# MIDI Files\n",
    "import mido\n",
    "import pretty_midi\n",
    "\n",
    "# torch utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d37f66-ce9c-4d60-9939-d19ecfa4a183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MIDI -> Piano Roll Array\n",
    "* An array of shape (128, time steps)\n",
    "* 128 representing notes from 0 to 127\n",
    "* Piano Roll to be converted to sequences (RNN/Transformer) or 2D format (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "473c01b3-c1e7-42c2-b638-a164d73d595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_music_list = os.listdir('classical_music')\n",
    "pop_music_list = os.listdir('pop_music')\n",
    "rock_music_list = os.listdir('rock_music')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "534ca691-e17b-4dd2-9fe9-91477accf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_music_list = [f\"classical_music/{p}\" for p in classical_music_list]\n",
    "pop_music_list = [f\"pop_music/{p}\" for p in pop_music_list]\n",
    "rock_music_list = [f\"rock_music/{p}\" for p in rock_music_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf218ea6-5dbc-4687-9760-7e035ae6632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_piano_roll(file_path, fs=100):\n",
    "    midi_data = pretty_midi.PrettyMIDI(file_path)\n",
    "    \n",
    "    # Convert to piano roll (time x notes)\n",
    "    piano_roll = midi_data.get_piano_roll(fs=fs)\n",
    "    \n",
    "    # Convert to binary (1 if note is played, 0 if not)\n",
    "    piano_roll_binary = (piano_roll > 0).astype(int)\n",
    "    return piano_roll_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f8fa68-9698-4a08-b8d0-27eb15f7f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franco/Documents/GitHub/AdvancedAI/venv/lib/python3.9/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pop_PR_list = list(map(midi_to_piano_roll, pop_music_list))\n",
    "classical_PR_list = list(map(midi_to_piano_roll, classical_music_list))\n",
    "rock_PR_list = list(map(midi_to_piano_roll, rock_music_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef76697-bc60-47c9-bf56-2b22563eb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 16217)\n",
      "(128, 23985)\n",
      "(128, 30787)\n",
      "(128, 17999)\n",
      "(128, 17333)\n",
      "(128, 9218)\n",
      "(128, 27078)\n",
      "(128, 15750)\n"
     ]
    }
   ],
   "source": [
    "for m in rock_PR_list:\n",
    "    print(m.shape)\n",
    "\n",
    "# 128 represents a note from 0 to 127, \n",
    "# Second item in the tuple represents how many time steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f401d-fb92-4c32-b04c-322b83bcc230",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Piano Roll -> Sequence\n",
    "* For RNNs or Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b0fbffad-61c0-406b-8c60-f9d1832a167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll2sequence(piano_roll, sequence_length=100):\n",
    "    # Check if piano roll has enough time steps for at least one sequence\n",
    "    if piano_roll.shape[1] < sequence_length:\n",
    "        return None\n",
    "    piano_roll_sequences = [piano_roll[:, i:i + sequence_length] \n",
    "                        for i in range(0, piano_roll.shape[1] - sequence_length, sequence_length)]\n",
    "    piano_roll_sequences = np.stack(piano_roll_sequences)\n",
    "    return piano_roll_sequences # shape (num_sequences, 128, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2737804-1db6-43c3-865d-34c41c497073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out `None` values in the sequence lists\n",
    "pop_sequences = list(filter(None, map(roll2sequence, pop_PR_list)))\n",
    "classical_sequences = list(filter(None, map(roll2sequence, classical_PR_list)))\n",
    "rock_sequences = list(filter(None, map(roll2sequence, rock_PR_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b43ba3aa-8d5e-44b0-a339-49c1f9741ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 128, 100)\n",
      "(239, 128, 100)\n",
      "(307, 128, 100)\n",
      "(179, 128, 100)\n",
      "(173, 128, 100)\n",
      "(92, 128, 100)\n",
      "(270, 128, 100)\n",
      "(157, 128, 100)\n"
     ]
    }
   ],
   "source": [
    "for s in rock_sequences:\n",
    "    print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a2bf2-4db9-499e-97f5-adb53ad55b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sequence -> Torch Tensor\n",
    "* For CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5153e065-3235-4599-b544-eaa3592184c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll2tensor(PR_sequences):\n",
    "    return torch.tensor(PR_sequences, dtype=torch.float32) # shape (num_sequences, 128, sequence, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ccbc83b-f2cd-4623-8a3b-38844784b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_tensors = list(map(roll2tensor, pop_sequences))\n",
    "classical_tensors = list(map(roll2tensor, classical_sequences))\n",
    "rock_tensors = list(map(roll2tensor, rock_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "638f1e2c-3173-4e63-918e-0177d0baa59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 128, 100])\n",
      "torch.Size([239, 128, 100])\n",
      "torch.Size([307, 128, 100])\n",
      "torch.Size([179, 128, 100])\n",
      "torch.Size([173, 128, 100])\n",
      "torch.Size([92, 128, 100])\n",
      "torch.Size([270, 128, 100])\n",
      "torch.Size([157, 128, 100])\n"
     ]
    }
   ],
   "source": [
    "for t in rock_tensors:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d40ab-7df3-4d9d-bf3e-a0ead0cf3086",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0402adb3-fabb-403e-8ca5-63605a30fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each list of tensors to a .pt file\n",
    "torch.save(pop_tensors, 'pop_tensors.pt')\n",
    "torch.save(classical_tensors, 'classical_tensors.pt')\n",
    "torch.save(rock_tensors, 'rock_tensors.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625b1b5-a663-4782-bed2-acf0fb0b704e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7271364-4e2d-4e1b-ae0c-a7bbc494f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the lists back later\n",
    "pop_tensors = torch.load('tensors_folder/pop_tensors.pt')\n",
    "classical_tensors = torch.load('tensors_folder/classical_tensors.pt')\n",
    "rock_tensors = torch.load('tensors_folder/rock_tensors.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de566e4f-275c-48b8-9845-f72641269dd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "989c21cb-7332-4db2-892f-879eed8255e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tensor_list):\n",
    "        self.data = tensor_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # Pads each sequence to the length of the longest sequence in this batch\n",
    "    batch = [item.squeeze() for item in batch]  # Remove extra dimensions if needed\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_batch\n",
    "\n",
    "# Each tensor in the music dataset will not have the same shape because shapes are (num_sequences, note=128, fs=100)\n",
    "# So we need to pad_collate each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b574da44-6aba-48b2-94b1-d0321c911220",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dataset = MusicDataset(pop_tensors)\n",
    "classical_dataset = MusicDataset(classical_tensors)\n",
    "rock_dataset = MusicDataset(rock_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fae0a58a-711e-41fa-baf2-11131f7cc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "pop_loader = DataLoader(pop_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "classical_loader = DataLoader(classical_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "rock_loader = DataLoader(rock_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72f84c00-e7c1-4fbc-988a-7e269120813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1321, 128, 100])\n",
      "torch.Size([32, 605, 128, 100])\n",
      "torch.Size([32, 630, 128, 100])\n",
      "torch.Size([32, 670, 128, 100])\n",
      "torch.Size([32, 392, 128, 100])\n",
      "torch.Size([32, 760, 128, 100])\n",
      "torch.Size([32, 687, 128, 100])\n",
      "torch.Size([32, 1011, 128, 100])\n",
      "torch.Size([32, 592, 128, 100])\n",
      "torch.Size([7, 830, 128, 100])\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(classical_loader):\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d837ab4c-c01b-4291-a04e-c45c116fb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 186, 128, 100])\n",
      "torch.Size([18, 99, 128, 100])\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(pop_loader):\n",
    "    print(batch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kernel",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
